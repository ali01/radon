\documentclass[12pt]{article}
\usepackage[top=1.1in, bottom=1.1in, left=1.1in, right=1.1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{listings}
\usepackage{color}
% \usepackage{times}
% \usepackage{pslatex}
% \usepackage{newcent}
\usepackage{palatino}
% \usepackage{palatcm}
\usepackage{float}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[pdftex]{graphicx}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}

\lstset{
   language=c++,
   basicstyle=\footnotesize,
   numbers=right,
   numberstyle=\footnotesize,
   stepnumber=2,
   numbersep=0pt,
   backgroundcolor=\color{white},
   showspaces=false,
   showstringspaces=false,
   showtabs=false,
   % frame=single,
   tabsize=3,
   captionpos=b,
   breaklines=true,
   breakatwhitespace=false,
   escapeinside={\%*}{*)}
}

\mathversion{normal}
\parindent 0pt                               % paragraph indentation
% \parskip 0.3ex plus 0.2ex minus 0.2ex             % paragraph spacing

\renewcommand{\labelenumi}{ (\alph{enumi})}
\renewcommand{\labelenumii}{\roman{enumii}.}

\newcommand{\e}[1]{\ensuremath{\times 10^{#1}}}
\newcommand{\answer}[1]{\textbf{Answer:} $\boxed{#1}$}
\newcommand{\spacer}{\\[8pt]}
\newcommand{\bkpd}{\\[-36pt]}
\newcommand{\pd}{\\[2pt]}

% ===================
% = Document Header =
% ===================
\begin{document}
   % \begin{onehalfspace}

   \begin{center}
      {\textbf{
         {\textit{
            % ==================
            % = Document Title =
            % ==================
            {\Large{CS 109 -- Problem Set 6}}
         }}
      }}\\[2pt]
      % =====================
      % = Document Subtitle =
      % =====================
      {\large{Ali Yahya --- ali01@stanford.edu --- 05416528}}
   \end{center}


   % =======================
   % = Section ~ Problem 1 =
   % =======================
   \section*{Problem 1}
      An estimate for the value of true $\mu$ from a sample mean, $\bar{X}$,
      a sample variance, $S^2$, and a number of trials, $n$, is given by the
      following expression (which evaluates to a range):
      \begin{align*}
         \left(
            \bar{X} - Z_{\alpha / 2} \left(\frac{S}{\sqrt{n}}\right),\;
            \bar{X} + Z_{\alpha / 2} \left(\frac{S}{\sqrt{n}}\right)
         \right)
      \end{align*}
      \begin{enumerate}
         \item $\alpha = 0.10$
            \begin{align*}
               \left(
                  447 - 1.645 \left(\frac{19.32}{\sqrt{300}}\right),\;
                  447 + 1.645 \left(\frac{19.32}{\sqrt{300}}\right)
               \right)
            \end{align*}
            \begin{align*}
               \boxed{\Big(445.165,\; 448.835\Big)}
            \end{align*}
         \item
            $\alpha = 0.05$
            \begin{align*}
               \left(
                  447 - 2.807 \left(\frac{19.32}{\sqrt{300}}\right),\;
                  447 + 2.807 \left(\frac{19.32}{\sqrt{300}}\right)
               \right)
            \end{align*}
            \begin{align*}
               \boxed{\Big(444.814,\; 449.186\Big)}
            \end{align*}
         \item
            $\alpha = 0.01$
            \begin{align*}
               \left(
                  447 - 2.807 \left(\frac{19.32}{\sqrt{300}}\right),\;
                  447 + 2.807 \left(\frac{19.32}{\sqrt{300}}\right)
               \right)
            \end{align*}
            \begin{align*}
               \boxed{\Big(444.127,\; 449.873\Big)}
            \end{align*}
      \end{enumerate}

      \newpage
      % =======================
      % = Section ~ Problem 2 =
      % =======================
      \section*{Problem 2}
         \begin{enumerate}
            \item
               Approach: compute log-likelihood, differentiate, set
               equal to zero, and and solve for lambda
               \begin{align*}
                  X_i &\sim Exp(\lambda) \pd
                  f(X_i | \lambda) &= \lambda \cdot e^{-\lambda \cdot x_i} \pd
                  L(\theta) &=
                     \prod_{i = 1}^{n} \lambda \cdot e^{-\lambda \cdot x_i} \pd
                  LL(\theta) &= \sum_{i = 1}^{n}
                     \ln \big(\lambda \cdot e^{-\lambda \cdot x_i}\big) \pd
                  &= \sum_{i = 1}^{n} \big(\ln(\lambda) - \lambda \cdot x_i) \pd
                  &= n \cdot \ln(\lambda) - \lambda \sum_{i=1}^n{x_i} \pd
                  \frac{\partial LL(\theta)}{\partial \lambda} &=
                     \frac{n}{\lambda} - \sum_{i = 1}^{n}{x_i} = 0 \pd
                  \lambda &= \frac{n}{\sum_{i=1}^{n}{x_i}}
               \end{align*}
            \item
               Approach: compute the expected value of the above estimate for
               lambda. Note that the expected value for any exponential random
               variable is equal to $\frac{1}{\lambda}$.
               \begin{align*}
                  E\left[\frac{n}{\sum_{i=1}^{n}{x_i}}\right] =
                     \frac{n}{\sum\limits_{i=1}^{n}{E[x_i]}}
                  = \frac{n}{\sum\limits_{i=1}^{n}{\frac{1}{\lambda}}}
                  = \frac{n}{n/\lambda}
                  = \lambda
               \end{align*}
               Because the expected value of our estimate for $\lambda$ is equal
               to $\lambda$, our estimate is unbiased.

               \answer{\text{No}}
            \item
               Given (from part \emph{b}) that our estimation for lambda is
               unbiased, we can be certain that, in the limit as $n$ approaches
               infinity, the value of our estimate is equal to the true
               lambda. Therefore, for large enough $n$, the absolute value of
               the difference between the true lambda and our estimate must be
               smaller than $\epsilon$, where $\epsilon$ is arbitrarily small.

               \answer{\text{Yes}}
         \end{enumerate}

      \newpage
      % =======================
      % = Section ~ Problem 3 =
      % =======================
      \section*{Problem 3}
         Consider the log odds of the probability of $X_i$ for $Y = 1$ and $Y = 0$.
         \begin{align*}
            \frac{
               P(Y=0)\prod\limits_{i = 1}^{n}{P(X_i | Y = 0)}
            }{
               P(Y=1)\prod\limits_{i = 1}^{n}{P(X_i | Y = 1)}
            }
         \end{align*}
         If the value of the expression above is greater than one, a Naive Bayes
         classifier would select $Y = 0$ as a prediction, and $Y = 0$ otherwise.\pd
         If we take the log of the expression above, we get the following:
         \begin{gather*}
            \log\left(
               \frac{
                  P(Y=0)\prod\limits_{i = 1}^{n}{P(X_i | Y = 0)}
               }{
                  P(Y = 1)\prod\limits_{i = 1}^{n}{P(X_i | Y = 1)}
               }
            \right) \pd
            \log\left(P(Y=0)\prod\limits_{i = 1}^{n}{P(X_i | Y = 0)}\right) -
            \log\left(P(Y=1)\prod\limits_{i = 1}^{n}{P(X_i | Y = 1)}\right) \pd
            \log{P(Y = 0)} + \sum_{i=1}^{n}{\log P(X_i | Y=0)} -
            \log{P(Y = 1)} - \sum_{i=1}^{n}{\log P(X_i | Y = 1)}
         \end{gather*}
         Because each variable $X_i$ appears in separate terms of the sum, the above
         is a linear function of the input variables.

      \newpage
      \section*{Programming Problem 1}
         \begin{enumerate}
            \item
               Prediction Results (simple-test.txt) $\sim$ Naive Bayes using MLE\\
               Class 0: tested 2, correctly classified 2\\
               Class 1: tested 2, correctly classified 2\\
               Overall: tested 4, correctly classified 4\\
               Accuracy = 1

               Prediction Results (simple-test.txt) $\sim$
                  Naive Bayes using Laplace Estimation\\
               Class 0: tested 2, correctly classified 2\\
               Class 1: tested 2, correctly classified 2\\
               Overall: tested 4, correctly classified 4\\
               Accuracy = 1

            \item
               Prediction Results (vote-test.txt) $\sim$ Naive Bayes using MLE\\
               Class 0: tested 52, correctly classified 48\\
               Class 1: tested 83, correctly classified 76\\
               Overall: tested 135, correctly classified 124\\
               Accuracy = 0.918519

               Prediction Results (vote-test.txt) $\sim$
                  Naive Bayes using Laplace Estimation\\
               Class 0: tested 52, correctly classified 48\\
               Class 1: tested 83, correctly classified 76\\
               Overall: tested 135, correctly classified 124\\
               Accuracy = 0.918519\

            \item
               Prediction Results (heart-test.txt) $\sim$ Naive Bayes using MLE \\
               Class 0: tested 15, correctly classified 10\\
               Class 1: tested 172, correctly classified 135\\
               Overall: tested 187, correctly classified 145\\
               Accuracy = 0.775401

               Prediction Results (heart-test.txt) $\sim$
                  Naive Bayes using Laplace Estimation\\
               Class 0: tested 15, correctly classified 10\\
               Class 1: tested 172, correctly classified 130\\
               Overall: tested 187, correctly classified 140\\
               Accuracy = 0.748663

         \end{enumerate}

         \newpage
         \section*{Programming Problem 2}
            \begin{enumerate}
               \item
                  Prediction Results (simple-test.txt) $\sim$ Logistic Regression\\
                  Class 0: tested 2, correctly classified 2\\
                  Class 1: tested 2, correctly classified 2\\
                  Overall: tested 4, correctly classified 4\\
                  Accuracy = 1

               \item
                  Prediction Results (vote-test.txt) $\sim$ Logistic Regression\\
                  Class 0: tested 52, correctly classified 51\\
                  Class 1: tested 83, correctly classified 83\\
                  Overall: tested 135, correctly classified 134\\
                  Accuracy = 0.992593

                  Logistic regression allows for more flexibility in modeling the
                  input data. Whereas Naive Bayes attempts to reverse engineer a
                  prediction from the joint probability distribution of $X$ and $Y$,
                  logistic regression models the probability of $Y$ given the data
                  directly.

               \item
                  Prediction Results (heart-test.txt) $\sim$ Logistic Regression\\
                  Class 0: tested 15, correctly classified 12\\
                  Class 1: tested 172, correctly classified 147\\
                  Overall: tested 187, correctly classified 159\\
                  Accuracy = 0.850267

               \item
                  With $10000$ epochs and learning rate of $1\e{-7}$

                  Prediction Results (heart-test.txt) $\sim$ Logistic Regression\\
                  Class 0: tested 15, correctly classified 7\\
                  Class 1: tested 172, correctly classified 168\\
                  Overall: tested 187, correctly classified 175\\
                  Accuracy = 0.935829

                  With a smaller step size toward the absolute maxima over the concave
                  log-conditional likelihood function can be determined with greater
                  precision. As a result, the prediction accuracy is greater.
            \end{enumerate}
\end{document}
